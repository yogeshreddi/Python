{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evalutaion Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try Knn with K = 10 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X,Y)\n",
    "knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using knn is 0.980000\n"
     ]
    }
   ],
   "source": [
    "## Looking at the training accuracy, which is our model evaluation metric here \n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Accuracy using knn is %f' %metrics.accuracy_score(knn.predict(X),Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using knn with k= 1 is 1.000000\n",
      "Accuracy using knn with k= 2 is 0.980000\n",
      "Accuracy using knn with k= 3 is 0.960000\n",
      "Accuracy using knn with k= 4 is 0.960000\n",
      "Accuracy using knn with k= 5 is 0.966667\n",
      "Accuracy using knn with k= 6 is 0.973333\n",
      "Accuracy using knn with k= 7 is 0.973333\n",
      "Accuracy using knn with k= 8 is 0.980000\n",
      "Accuracy using knn with k= 9 is 0.980000\n",
      "Accuracy using knn with k= 10 is 0.980000\n",
      "Accuracy using knn with k= 11 is 0.973333\n",
      "Accuracy using knn with k= 12 is 0.980000\n",
      "Accuracy using knn with k= 13 is 0.980000\n",
      "Accuracy using knn with k= 14 is 0.980000\n",
      "Accuracy using knn with k= 15 is 0.986667\n",
      "Accuracy using knn with k= 16 is 0.986667\n",
      "Accuracy using knn with k= 17 is 0.980000\n",
      "Accuracy using knn with k= 18 is 0.973333\n",
      "Accuracy using knn with k= 19 is 0.980000\n",
      "Accuracy using knn with k= 20 is 0.980000\n",
      "Accuracy using knn with k= 21 is 0.980000\n",
      "Accuracy using knn with k= 22 is 0.980000\n",
      "Accuracy using knn with k= 23 is 0.980000\n",
      "Accuracy using knn with k= 24 is 0.973333\n",
      "Accuracy using knn with k= 25 is 0.980000\n",
      "Accuracy using knn with k= 26 is 0.973333\n",
      "Accuracy using knn with k= 27 is 0.973333\n",
      "Accuracy using knn with k= 28 is 0.966667\n",
      "Accuracy using knn with k= 29 is 0.973333\n"
     ]
    }
   ],
   "source": [
    "### let try with different k values \n",
    "\n",
    "k_range = range(1,30)\n",
    "k_score = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X,Y)\n",
    "    print('Accuracy using knn with k= %s is %f' %(k,metrics.accuracy_score(knn.predict(X),Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try logistic and DT not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using DT is 1.000000\n",
      "Accuracy using logistic is 0.960000\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X,Y)\n",
    "DT.predict(X)\n",
    "\n",
    "print('Accuracy using DT is %f' %metrics.accuracy_score(DT.predict(X),Y))\n",
    "\n",
    "### Logistic \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X,Y)\n",
    "logreg.predict(X)\n",
    "\n",
    "print('Accuracy using logistic is %f' %metrics.accuracy_score(logreg.predict(X),Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Knn with k = 1 is one of the best models we achieved but with a low value of k our model is trying to overfit the data and making our model complex, whihc is not ideal for our prediction.\n",
    "##### 2. Out of the three models DT is perfomring the best, whihc could be by chance and we dont have an estimation of how these models are perofrming on unseen data. \n",
    "##### 3. Overfiting could be a problem (Complex models will over fit the model, example knn with k = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Splitting / Validation Set Approach / Test Set Approach\n",
    "\n",
    "##### 1. Split the data into Training and test samples\n",
    "##### 2. Train the model using the training set and calculate the tetsing accuracy by the fitting the modle on test data.\n",
    "\n",
    "##### Advantages - K times faster that Kfold CV, Simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMporting required data,methods,metric\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the iris Data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Splitting the data into training and testing data sample\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.4,random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using KNN Classifier with K = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,Y_train)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n",
      "[[20  0  0]\n",
      " [ 0 21  2]\n",
      " [ 0  0 17]]\n"
     ]
    }
   ],
   "source": [
    "#Checking for Accuacy\n",
    "print(metrics.accuracy_score(y_pred,Y_test))\n",
    "print(metrics.confusion_matrix(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using knn with k= 1 is 0.966667\n",
      "Accuracy using knn with k= 2 is 0.916667\n",
      "Accuracy using knn with k= 3 is 0.983333\n",
      "Accuracy using knn with k= 4 is 0.966667\n",
      "Accuracy using knn with k= 5 is 0.966667\n",
      "Accuracy using knn with k= 6 is 0.966667\n",
      "Accuracy using knn with k= 7 is 0.983333\n",
      "Accuracy using knn with k= 8 is 0.983333\n",
      "Accuracy using knn with k= 9 is 1.000000\n",
      "Accuracy using knn with k= 10 is 0.983333\n",
      "Accuracy using knn with k= 11 is 1.000000\n",
      "Accuracy using knn with k= 12 is 0.983333\n",
      "Accuracy using knn with k= 13 is 1.000000\n",
      "Accuracy using knn with k= 14 is 0.983333\n",
      "Accuracy using knn with k= 15 is 1.000000\n",
      "Accuracy using knn with k= 16 is 0.966667\n",
      "Accuracy using knn with k= 17 is 0.983333\n",
      "Accuracy using knn with k= 18 is 0.966667\n",
      "Accuracy using knn with k= 19 is 0.966667\n",
      "Accuracy using knn with k= 20 is 0.950000\n",
      "Accuracy using knn with k= 21 is 0.966667\n",
      "Accuracy using knn with k= 22 is 0.966667\n",
      "Accuracy using knn with k= 23 is 0.966667\n",
      "Accuracy using knn with k= 24 is 0.966667\n",
      "Accuracy using knn with k= 25 is 0.966667\n",
      "Accuracy using knn with k= 26 is 0.950000\n",
      "Accuracy using knn with k= 27 is 0.950000\n",
      "Accuracy using knn with k= 28 is 0.950000\n",
      "Accuracy using knn with k= 29 is 0.950000\n"
     ]
    }
   ],
   "source": [
    "### Lets try the same for differen values of K, to find the best value of K\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,Y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print('Accuracy using knn with k= %s is %f' %(k,metrics.accuracy_score(y_pred,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n",
      "[[12  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  2 12]]\n",
      "0.947368421053\n",
      "[[15  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  2 11]]\n",
      "1.0\n",
      "[[16  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "#### Lets try all these by change the random state varibale \n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 5)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_pred,Y_test))\n",
    "print(metrics.confusion_matrix(y_pred,Y_test))\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 3)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_pred,Y_test))\n",
    "print(metrics.confusion_matrix(y_pred,Y_test))\n",
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 2)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_pred,Y_test))\n",
    "print(metrics.confusion_matrix(y_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you look at the above results, we are getting different results for different random_state (I.e. for different samples, so train/test split is not always reliable as it can give reults by chance and there is very high chance of the results being inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To get rid of the above problem, the solution we can think of is to try a bunch of train/test splits and avergaing the results produced.. That should be a good idea.. as the probablity of getting close to accurate results is higher in that case.. (as we are avergaing different combinations of results)\n",
    "\n",
    "##### Advantages - Accurate and efficient usage of data available.\n",
    "\n",
    "##### Reccomendation - Use K = 10 in Genral, for classification probelm use stratified sampling for creating folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Kfold CV\n",
    "##### 1. Split the Data Set into K equal Partitions\n",
    "##### 2. Use one of the fold as testing set and union of remaining sets as training sets to train your model\n",
    "##### 3. Calculate Testing accuracy\n",
    "##### 4. Repeat 2,3 using each of the remaing K-1 Folds as the testing sample and union of the other remaining sampples as training sample.\n",
    "##### 5. Calculate the Testing accuracy of each case and evaluate average of all the Testing accuracies as the estimate of over-all testing accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.93333333  1.          1.          0.86666667  0.93333333\n",
      "  0.93333333  1.          1.          1.        ]\n",
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# Lets try the Kfold CV on the same iris data using 5 folds, I.e. K=10\n",
    "# Our goal is to select the best tuning parameters\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "# Lets try KNN Classifier with K = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn,X,Y,cv=10,scoring = 'accuracy')\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95999999999999996, 0.95333333333333337, 0.96666666666666656, 0.96666666666666656, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.98000000000000009, 0.96666666666666656, 0.96666666666666656, 0.97333333333333338, 0.95999999999999996, 0.96666666666666656, 0.95999999999999996, 0.96666666666666656, 0.95333333333333337, 0.95333333333333337, 0.95333333333333337, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.94666666666666666, 0.95333333333333337]\n"
     ]
    }
   ],
   "source": [
    "#Searching for a optimal value of K\n",
    "\n",
    "K_Range = range(1,40)\n",
    "K_Scores = []\n",
    "for K in K_Range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    scores = cross_val_score(knn,X,Y,cv=10,scoring = 'accuracy')\n",
    "    K_Scores = K_Scores+[scores.mean()]\n",
    "    ## OR ## K_Scores.append(scores.mean())\n",
    "print(K_Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above results suggest that we get optimal cv score at K = 13,18,20 we will go with K = 13 thus decreasing our model complexity\n",
    "##### as its difficult to visulaize looking at the data, lets try plotting a graph K_Rnage vs K_Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19973872400>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngOCccw6A/w9ZUv0QYIohRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x199737ea0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(K_Range,K_Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now try comparing differnt models using KFold CV Logestic Reg VS KNN\n",
    "\n",
    "##### Goal is to compare the best KNN model with logestic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CV Score is 0.980000\n"
     ]
    }
   ],
   "source": [
    "# Best KNN Model\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "print('KNN CV Score is %f' %cross_val_score(knn,X,Y,cv = 10,scoring='accuracy').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic CV Score is 0.953333\n"
     ]
    }
   ],
   "source": [
    "# 10 fold Logistic model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "print('Logistic CV Score is %f' %cross_val_score(log_reg,X,Y,cv = 10,scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic CV Score for folds = 2 is 0.946667\n",
      "Logistic CV Score for folds = 3 is 0.946895\n",
      "Logistic CV Score for folds = 4 is 0.952457\n",
      "Logistic CV Score for folds = 5 is 0.960000\n",
      "Logistic CV Score for folds = 6 is 0.952160\n",
      "Logistic CV Score for folds = 7 is 0.952381\n",
      "Logistic CV Score for folds = 8 is 0.958333\n",
      "Logistic CV Score for folds = 9 is 0.951852\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,10):\n",
    "    print('Logistic CV Score for folds = %s is %f' %(i,cross_val_score(log_reg,X,Y,cv = i,scoring='accuracy').mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IN the above results we can see that the best possible result of logestic is with K= 5 fold cv whihc is still less accurate tha best KNN model, so we can conclude that KNN is better of the two for iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
